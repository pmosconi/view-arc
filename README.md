# View Arc Obstacle Detection

Finds the obstacle with largest visible angular coverage within a field-of-view arc from a viewer point.

## Installation

```bash
uv venv --python 3.13
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -e .
```

For development:
```bash
uv pip install -e ".[dev]"
```

## Usage

The quickest way to see the API in action is to run the bundled basic example:

```bash
uv run python examples/basic_usage.py
```

This prints a human-readable summary generated by the richly documented
``ObstacleResult`` class. You can also copy the minimal pattern below into
your own project:

```python
import numpy as np
from view_arc import find_largest_obstacle

viewer = np.array([100.0, 100.0], dtype=np.float32)
view_direction = np.array([0.0, 1.0], dtype=np.float32)  # unit vector
field_of_view_deg = 60.0
max_range = 150.0
contours = [
    np.array([[90.0, 150.0], [110.0, 150.0], [100.0, 190.0]], dtype=np.float32),
    np.array([[70.0, 130.0], [130.0, 130.0], [130.0, 180.0], [70.0, 180.0]], dtype=np.float32),
]

result = find_largest_obstacle(
    viewer_point=viewer,
    view_direction=view_direction,
    field_of_view_deg=field_of_view_deg,
    max_range=max_range,
    obstacle_contours=contours,
    return_intervals=True,
)

print(result.summary())
```

All public functions in the package include comprehensive docstrings that you
can explore via ``help(view_arc.find_largest_obstacle)``.

## Examples

### Single-Frame Obstacle Detection

- `examples/basic_usage.py` – minimal, self-contained invocation with console output.
- `examples/visualization_demo.py` – renders the wedge, obstacles, and resolved intervals to `examples/output/visualization_demo.png` (requires OpenCV).
- `examples/real_image_processing.py` – extracts contours from the `skimage` astronaut image, runs detection, and saves an annotated overlay.

### Temporal Attention Tracking

- `examples/attention_tracking_basic.py` – minimal batch tracking example (10 seconds of viewer samples)
  - Shows basic usage of `compute_attention_seconds()`
  - Prints per-AOI hit counts and top AOIs
  - **Start here** for attention tracking

- `examples/attention_tracking_visualization.py` – heatmap visualization of attention distribution
  - Simulates 100-second viewing session
  - Generates colored heatmaps (hot/viridis colormaps)
  - Adds labels with hit counts and percentages
  - Saves outputs to `examples/output/`

- `examples/attention_tracking_analysis.py` – result analysis and export
  - Demonstrates all aggregation methods (`get_top_aois`, `get_attention_distribution`, etc.)
  - Exports results to pandas DataFrame
  - Shows viewing timeline and session statistics
  - Includes DataFrame operations examples

- `examples/simulated_store_session.py` – complete realistic simulation
  - Loads real store layout and AOI annotations from JSON
  - Generates 60 seconds of realistic browsing behavior
  - Simulates natural walking patterns and view scanning
  - Produces comprehensive visualizations (heatmap + timeline + path overlay)
  - **Most complete example** showing full workflow

Run every example with `uv run python <script>` so that dependencies resolve inside the project environment.

## Tracking Assumptions

Temporal attention tracking (see `docs/TRACKING_PLAN.md`) relies on the following upstream guarantees that we do **not** re-validate at runtime:

1. **1 Hz cadence**: Viewer samples arrive exactly at 1 Hz, so each accepted hit represents one second of attention.
2. **One second per sample**: Each sample represents exactly one second of viewing time—no interpolation is performed.
3. **Monotonic timestamps**: Timestamps, when provided, are already sorted upstream; we consume them as-is.
4. **Fixed coordinate space**: Viewer positions, directions, and AOI contours all share the same immutable image-coordinate space for the entire batch.
5. **Single viewer**: Each batch tracks a single viewer; multi-viewer aggregation happens outside this API.

Any pipeline feeding `compute_attention_seconds()` must uphold these invariants to keep the reported metrics meaningful. You can also inspect `TrackingResult.assumptions` or the `SAMPLING_ASSUMPTIONS` constant to see these contracts programmatically.

## Type Checking

Mypy is configured with ``disallow_untyped_defs`` and must remain clean:

```bash
uv run mypy .
```

Address any type errors before committing changes.

## Performance Profiling

The project includes performance profiling tools to monitor runtime and accuracy regression:

### Running Profiling Scenarios

Run performance baselines with:

```bash
# Run tracking baseline scenarios only
uv run python profile_workload.py --scenario tracking_baseline

# Run all scenarios (obstacle detection + tracking)
uv run python profile_workload.py --scenario all

# Save results to CSV for trend tracking
uv run python profile_workload.py --scenario tracking_baseline --save-csv
```

Results are saved to `examples/output/profile_runs.csv` with timestamp, runtime, throughput, and accuracy metrics.

### Profiling in Code

Enable lightweight profiling in tracking runs:

```python
from view_arc.tracking import compute_attention_seconds

result = compute_attention_seconds(
    samples=samples,
    aois=aois,
    enable_profiling=True  # Enable performance metrics
)

# Access profiling data
if result.profiling_data:
    print(result.profiling_data)
    # Output includes:
    # - Total time
    # - Samples processed
    # - Throughput (samples/s)
    # - Average time per sample (ms)
```

**Note**: Profiling has negligible overhead (<1%) and does not alter results.

### CI Integration

For weekly regression detection, run:

```bash
uv run python profile_workload.py --scenario tracking_baseline --save-csv
```

Compare results against previous runs in `examples/output/profile_runs.csv`. Threshold alerts are emitted when:
- Runtime exceeds 120% of golden baseline
- Hit counts differ from expected values

## Algorithm

See `docs/obstacle_arc_spec.md` for detailed algorithm specification.
